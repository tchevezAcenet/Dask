{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5b1c8b-f38c-4cb8-a1cb-2f31fc7b31e7",
   "metadata": {},
   "source": [
    "# Parallel Programming with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6daca0-b46e-44ea-8358-2a2dc1e6bb6b",
   "metadata": {},
   "source": [
    "Dask is a free Python tool designed to handle parallel computing by extending familiar libraries like Pandas, NumPy, and Scikit-Learn. It allows tasks to be distributed across multiple CPU cores or even clusters of computers, making it ideal for managing large datasets and complex calculations that exceed the memory capacity of a single machine. With a dynamic task scheduler and a focus on user-friendliness, Dask simplifies the process of scaling from single-machine operations to distributed computing, making advanced data processing more accessible and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c73c89-7c8f-420b-8634-b2db799763cd",
   "metadata": {},
   "source": [
    "This is how Dask works:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dask/dask/main/docs/source/images/dask-overview.svg\" \n",
    "     width=\"75%\"\n",
    "     alt=\"Dask overview\\\" />\n",
    "\n",
    "*Source: Coiled Dask Tutorial (https://github.com/coiled/dask-mini-tutorial/tree/main)*\n",
    "\n",
    "In Dask, large datasets are represented as collections like Dask DataFrames, Dask Arrays, etc. Operations on these collections don't compute everything right away, instead they are translated into a task graph. This task graph is a map of small, independent computations (tasks) connected by the data they produce and consume. Finally, a scheduler decides how to efficiently run these tasks across multiple cores or machines.\n",
    "\n",
    "Imagine you have a giant puzzle that won't fit on your table, so you break it into smaller pieces. Dask treats big data like that puzzle. It splits the data into manageable chunks. Then, instead of immediately solving the entire puzzle, Dask makes a plan of what needs to be done for each chunk. This plan is like a task graph, showing how the smaller jobs connect. Finally, Dask uses a scheduler, like a smart foreman, to assign these tasks to different workers (cores on your computer) to solve them all simultaneously, finishing the giant puzzle much faster.\n",
    "\n",
    "### Dask Task Graphs\n",
    "Dask relies on task graphs to represent and manage computations. A task graph is a directed acyclic graph (DAG) that describes the sequence of operations and dependencies in a computation. \n",
    "\n",
    "Components of a Dask Task Graph:\n",
    "- Tasks:\n",
    "    - Tasks are units of computation represented by individual operations on the data, such as arithmetic operations, function applications, or reductions.\n",
    "- Dependencies:\n",
    "     - Dependencies define the relationships between tasks, indicating which tasks need to be completed before others can begin.\n",
    "- Graph:\n",
    "    - The entire task graph is a collection of interconnected tasks and dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9536c3-3d25-49f5-82e9-332739d4aa03",
   "metadata": {},
   "source": [
    "## Dask Delayed\n",
    "Dask delayed is a powerful tool in the Dask library that allows you to parallelize your existing Python code. It operates by creating delayed objects, which encapsulate function calls and their parameters as a task graph. This graph represents the computation to be performed and can be executed efficiently in a parallel or distributed manner.\n",
    "\n",
    "Dask implements lazy evaluation as a fundamental concept to efficiently manage and execute computations on large datasets. The core idea behind lazy evaluation is to delay the actual computation until the result is explicitly needed. \n",
    "\n",
    "Dask accomplishes this through the following key mechanisms:\n",
    "\n",
    "### Delayed Objects\n",
    "Dask utilizes the **dask.delayed** module to create delayed objects, which represent computations to be executed later. These delayed objects capture the operations or functions applied to data but don't execute them immediately. Instead, they construct a task graph. The *dask.delayed* decorator (**@delayed**) serves as a transformative tool, converting normal Python functions into delayed functions. \n",
    "\n",
    "Lets work on an example where we will create a delayed function and delayed objects where we add numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dee0bbc5-481b-4b95-b414-2b817ec29e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ed8b0-8ffd-4f98-8b0a-cf82ae2f3c21",
   "metadata": {},
   "source": [
    "When we run this code, we see that we don't obtain the result value right away, as we would normally expect. Instead, it returns the delayed object.\n",
    "\n",
    "Dask's **delayed** is a way to create computations lazily, meaning that when you define 'result', Dask doesn't actually perform the addition operations right away. Instead, it constructs a task graph that describes what operations should be done and how they depend on each other.\n",
    "\n",
    "In our code, **add(1, 2)** and **add(3, 4)** are both ***delayed operations***. When we perform **add(1, 2) + add(3, 4)**, it creates a new delayed object that represents the sum of the results of these two delayed calls, not the actual integer result.\n",
    "\n",
    "To actually execute the computation and get the result, we need to use the **.compute()** method on the final result object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bd55f29-cb9a-417b-ac52-0dfd82bd3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdff6c-a894-4dcd-b774-63252a832f69",
   "metadata": {},
   "source": [
    "Now, lets visualize how the task graph looks like.\n",
    "\n",
    "First, go to your terminal and run the following command:\n",
    "***conda install python-graphviz***\n",
    "\n",
    "This will install graphviz which is a tool for creating and displaying diagrams of connected nodes, and in JupyterLab, it's used to visualize the steps and dependencies in a series of tasks, like those created by Dask, to help you understand how your computations are organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "117b4e76-6dd8-4de5-bc55-d4d6d0190102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing a task graph\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0348f-d3c0-4f4a-82b2-a9ca57ea62fb",
   "metadata": {},
   "source": [
    "**Explanation of the Task Graph:**\n",
    "\n",
    "*Tasks:*\n",
    "- add(1, 2): This creates a task to add the numbers 1 and 2.\n",
    "- add(3, 4): This creates a task to add the numbers 3 and 4.\n",
    "\n",
    "*Dependency:*\n",
    "- The result of add(1, 2) and add(3, 4) are then added together to form the final result. This addition itself is a task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b69dd-3223-4960-9d29-3afeca8007e6",
   "metadata": {},
   "source": [
    "Let's work on another *dask.delayed* example in which we will get the square values of various numbers and then get the sum result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dd4624b-75ff-41da-8a77-9ecdff5e09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create delayed functions\n",
    "#Your code goes here\n",
    "\n",
    "# Create delayed objects\n",
    "#Your code goes here\n",
    "\n",
    "# Create a delayed object for adding the results\n",
    "#Your code goes here\n",
    "\n",
    "# Compute the result\n",
    "#Your code goes here\n",
    "\n",
    "#Visualize the result\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac42f6-be34-4c37-b6a5-a614e624d81a",
   "metadata": {},
   "source": [
    "- **@delayed** decorates the functions square and sum_list, transforming them into delayed functions.\n",
    "- **square(x)**: A delayed function that squares its input.\n",
    "- **sum_list(values)**: A delayed function that calculates the sum of a list of values.\n",
    "- **a = square(2), b = square(3), c = square(4)**: Creates delayed objects by calling the square function with different arguments. These represent the squared values of 2, 3, and 4. The delayed objects a, b, and c form a task graph. This graph captures the dependencies between computations and represents the sequence of operations to be performed.\n",
    "- **result = sum_list([a, b, c])** creates a new delayed object that represents the sum of the squared values. The list [a, b, c] is passed as an argument, and the delayed summation function encapsulates this computation.\n",
    "- **final_result = result.compute()** initiates the computation of the delayed object result. This triggers the execution of the task graph, computing the sum of the squared values.\n",
    "- **print(final_result)** outputs the final result of the computation, which is the sum of the squared values of 2, 3, and 4.\n",
    "- **result.visualize()** : Shows the task graph of the delayed 'result'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11a93d-e09c-41b3-8f45-b5e504480bbd",
   "metadata": {},
   "source": [
    "### Exercise 1 (Breakout rooms)\n",
    "- Write Dask delayed functions:\n",
    "    - ***compute_square*** : returns the square of a given number\n",
    "    - ***compute_cube*** : returns the cube of a given number\n",
    "\n",
    "- Create ***delayed objects*** for computing the square and cube of a given number.\n",
    "- Create a ***delayed object*** that depends on the results of both functions to calculate their sum.\n",
    "- Visualize the task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5402f850-2027-44a1-ba5a-10c08bfe4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the square of a number\n",
    "#Your code goes here\n",
    "\n",
    "# Function to compute the cube of a number\n",
    "#Your code goes here\n",
    "\n",
    "# Given number\n",
    "number = #Your code goes here\n",
    "\n",
    "#Create delayed objects\n",
    "#Your code goes here\n",
    "\n",
    "# Create the delayed sum object\n",
    "#Your code goes here\n",
    "\n",
    "# Step 4: Compute and print the final result\n",
    "#Your code goes here\n",
    "\n",
    "#Visualize the task graph\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52d01e-8a6e-46d0-9443-f58464a5f5c2",
   "metadata": {},
   "source": [
    "## Dask Arrays\n",
    "So far we have explored Dask's **dask.delayed** functionality for parallelizing functions. However, Dask offers a powerful collection called Dask arrays that already include a parallel version of multiple functions.\n",
    "\n",
    "These arrays act like familiar NumPy arrays, but under the hood, they leverage Dask's distributed computing capabilities. This means Dask arrays can efficiently handle large datasets that wouldn't fit in memory on a single machine. Dask automatically splits the data into smaller chunks and processes them in parallel across multiple cores or a cluster, significantly speeding up computations on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3597b3-a9eb-4454-823f-e5fda1e0f782",
   "metadata": {},
   "source": [
    "### Creating a Dask Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "258201d3-ecac-492f-bada-4951e61b986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dask array\n",
    "#Your code goes here\n",
    "\n",
    "# Creating a Dask array with ones\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "532a2674-9e85-46c1-bbc8-dc663f297ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eec768-4294-4c8e-a4eb-220d6d62abab",
   "metadata": {},
   "source": [
    "**Code explanation:**\n",
    "- **import dask.array as da** imports the dask.array module and gives it the alias da. This module provides functionality for creating and manipulating Dask arrays.\n",
    "- **x = da.ones((1000, 1000), chunks=(100, 100))** Dask array x is created using the da.ones function. This function generates an array filled with ones. The array has a shape of (1000, 1000), meaning it is a 2D array with 1000 rows and 1000 columns. The chunks=(100, 100) argument specifies how the array should be chunked for parallel processing. In this case, it is divided into chunks of size 100x100. Chunking is a crucial aspect of Dask that allows for parallel and distributed computing on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5e3ea-e727-4ff7-ae99-23f28e9b2040",
   "metadata": {},
   "source": [
    "### Basic Array Operations\n",
    "\n",
    "#### Performing Element-Wise Operations\n",
    "Dask arrays support a wide range of element-wise operations similar to NumPy. These operations are applied independently to each element in the array, allowing for parallelized computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34286e44-f924-480d-9412-51aabbacd78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dask arrays\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5aaea0c4-ff09-4e62-a0e2-41d90590b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8e8c7-8f1a-4af6-a60d-510aba6f4f0d",
   "metadata": {},
   "source": [
    "In this example, we create a Dask array x filled with ones. The element-wise addition operation x + 5 is performed, creating a new Dask array y. The operation is applied to each element of the array independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bf304-1106-4826-818c-6b430ef5788f",
   "metadata": {},
   "source": [
    "#### Statistical Operations\n",
    "Dask arrays provide efficient ways to compute basic statistics on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb179c0e-01df-4c04-a803-bfadbb7ecbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing mean along axis 0\n",
    "#Your code goes here\n",
    "\n",
    "# Computing sum along axis 1\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297dd79-40e2-4505-9674-57c76f0c851b",
   "metadata": {},
   "source": [
    "In this example, we compute the mean along axis 0 (columns) and the sum along axis 1 (rows) of the Dask array x. These operations are performed in parallel across chunks, showcasing the scalability of Dask arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaced3b-f3bd-4836-aad2-8add031d0011",
   "metadata": {},
   "source": [
    "### Advanced Array Operations\n",
    "\n",
    "#### Concatenation\n",
    "Concatenation involves combining multiple arrays along a specified axis. We can use Dask's **da.concatenate** function to do this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed4e7b14-7f78-417d-837c-215f5e89baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two Dask arrays\n",
    "#Your code goes here\n",
    "\n",
    "# Concatenating\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7facb6-9c35-460b-9eba-9bbd5fbe5b13",
   "metadata": {},
   "source": [
    "Here, we create two Dask arrays, a and b, and concatenate them along axis 1. The resulting concatenated_result has doubled its width."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7109a-cf8b-4d97-9585-e1c4d8da297c",
   "metadata": {},
   "source": [
    "#### Stacking\n",
    "Stacking involves combining arrays along a new axis. We can use Dask's **da.stack** function to stack arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ec663ba-fcc3-46f5-83b0-4877cb98be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two Dask arrays\n",
    "#Your code goes here\n",
    "\n",
    "# Stacking along a new axis\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c1911-8324-4644-9cd4-73eb0e42cd2a",
   "metadata": {},
   "source": [
    "Here, we create two Dask arrays, c and d, and stack them along a new axis (axis 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e4adb-c836-4a5e-9bf4-d02a244d125f",
   "metadata": {},
   "source": [
    "#### Indexing and Slicing\n",
    "Dask arrays support efficient indexing and slicing, allowing you to access and extract subarrays of specific elements or regions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53af884b-e49d-47b8-8eea-5ce1b1f0f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dask array\n",
    "#Your code goes here\n",
    "\n",
    "# Select first 200 elements from first row\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfc9508e-c482-4d25-8d8d-82006fdbca0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select entire second column\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fd5ed3d-a216-466c-9d49-bff2953cdf55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a sub-array of size (5, 3) starting from (20, 40)\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1beff0d6-2ef0-4217-8ae9-72201b393ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select elements greater than 5\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d15c3cd0-bb3f-46f0-a2a0-8f9a060473a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access element at (10, 50)\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b55e22-a1bb-4f2d-935d-91651e90fbca",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "\n",
    "Dask arrays are often divided into chunks for parallel processing, just like in this case. While you can access elements using plain integer indexing like ***x[10, 50]***, it might not be the best and most efficient move.\n",
    "\n",
    "Dask might need extra work to find the element within chunks, especially near chunk boundaries. To avoid this overhead and ensure clarity, use slicing with a step size of 1: **x[10:11, 50:51]**. This tells Dask exactly what you want (a single element) and helps it efficiently retrieve the data you need. So, for clear and efficient element access in Dask arrays, embrace slicing with a step of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd99d75-46e5-4f10-b795-e6b1de32b96a",
   "metadata": {},
   "source": [
    "## Exercise 2 (Breakout rooms)\n",
    "\n",
    "- **Create a Dask Array:** Generate a Dask array with random integers of size (10000, 10000) and chunks (1000, 1000).\n",
    "- **Basic Operations:**\n",
    "    - Perform addition by adding 5 to every element of the Dask array.\n",
    "    - Perform multiplication by multiplying every element of the Dask array by 2.\n",
    "- **Indexing:**\n",
    "    - Select and print specific elements:\n",
    "        - Select the element at row 0, column 0.\n",
    "        - Select the element at row 2, column 3.\n",
    "- **Slicing:**\n",
    "    - Slice the Dask array to select specific rows and columns:\n",
    "        - Select rows 1 to 3 and all columns.\n",
    "        - Select all rows and columns 2 to 3.\n",
    "- **Print Results:** Print the original Dask array and the results of the operations, indexing, and slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b85cac6-d42e-492c-b004-84af9fd14dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a Dask array\n",
    "#Your code goes here\n",
    "\n",
    "# Step 2: Basic Operations\n",
    "# Addition\n",
    "#Your code goes here\n",
    "\n",
    "# Multiplication\n",
    "#Your code goes here\n",
    "\n",
    "# Step 3: Indexing\n",
    "# Select and print specific elements\n",
    "#Your code goes here\n",
    "\n",
    "# Step 4: Slicing\n",
    "# Select specific rows and columns\n",
    "#Your code goes here\n",
    "\n",
    "# Print results\n",
    "#Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e268e-4672-4205-8fb4-ed037326f240",
   "metadata": {},
   "source": [
    "## Dask Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b8e0b-f20b-461a-b6af-b3a65472d8c3",
   "metadata": {},
   "source": [
    "While Dask's dask.delayed allows us to parallelize individual functions, a more powerful tool exists: Dask DataFrames.\n",
    "\n",
    "Similar to pandas DataFrames, Dask DataFrames provide a familiar interface for working with tabular data. However, unlike pandas, Dask DataFrames are designed to handle massive datasets that wouldn't fit in memory on a single machine. They achieve this by splitting the data into smaller partitions and distributing them across multiple cores or a cluster for parallel processing. This parallel processing significantly accelerates computations on large datasets. Dask DataFrames offer many familiar functionalities from pandas, allowing you to perform operations like filtering, joining, and aggregations efficiently on big tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a32c58d-13a1-4870-9a28-7cd6d0bbda6b",
   "metadata": {},
   "source": [
    "## When to use Dask instead of Pandas\n",
    "\n",
    "Dask enhances popular PyData libraries such as Pandas, Numpy, and scikit-learn by introducing parallel computing capabilities.\n",
    "\n",
    "**Limitations of Pandas:**\n",
    "Pandas is widely used because it is highly effective and efficient for data manipulation and analysis in Python. However, it has a significant limitation: it can only handle relatively small datasets that fit into memory. A common rule of thumb is that your dataset should be no more than 20% of your available RAM. When the dataset exceeds this size, Pandas can run into memory errors because it tries to load all the data into memory at once.\n",
    "\n",
    "**How Dask Enhances Pandas:**\n",
    "The creators of Dask wanted to improve this situation without changing the familiar and convenient Pandas interface. They developed Dask to scale up the capabilities of Pandas, allowing it to handle larger datasets. Dask achieves this by dividing large datasets into smaller, manageable chunks and processing these chunks in parallel across multiple CPU cores.\n",
    "\n",
    "**Using Dask for Large Datasets**\n",
    "For example, if you have a dataset with 665 million rows, loading it with Pandas would likely result in a memory error due to insufficient RAM. However, with Dask, you can load and process this large dataset easily. The Dask syntax is very similar to Pandas, making it easy for users to transition and work with larger data.\n",
    "\n",
    "**Efficient Use of CPU Cores**\n",
    "One of the main advantages of Dask is its ability to perform parallel computing. While Pandas processes all computations on a single core, Dask distributes the workload across multiple CPU cores. This parallel processing ensures that all available computational resources are utilized, significantly speeding up data processing.\n",
    "\n",
    "**Distributed Processing**\n",
    "Additionally, Dask can scale beyond a single machine. The same code that works on a single machine can be used to distribute processing across multiple machines in a cluster. This allows Dask to handle even larger datasets and more complex computations by leveraging the combined power of multiple machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c18f5c-2ddc-4da3-8b52-2a99cef5d6b2",
   "metadata": {},
   "source": [
    "Let's try loading an extremely large dataset of 665 million rows using pandas and Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ede6da-f4ed-4425-b930-dec023b2eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#DO NOT RUN THIS CODE IN CLASS (will take too long to load)\n",
    "df = pd.read_parquet(\"big_file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85102ab2-e239-4d09-8eea-26b3211829ef",
   "metadata": {},
   "source": [
    "If you try this code, after a very long time trying to process it, it will give you a \"Kernel Restarting\" error message which is considered a memory error as our machine does not have enough RAM to load this in.\n",
    "\n",
    "However, if we try it with Dask, it will be loaded easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83e1a4ed-38ea-48d0-b893-82626a728751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3400de4-290d-430c-b5e2-1b4d10c87faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92a3b0-4ff2-482b-a131-faa0fcf193f8",
   "metadata": {},
   "source": [
    "This code uses Dask to calculate the number of unique values in the \"id\" column of a Dask DataFrame. The **%%time** magic command in Jupyter Notebooks measures and outputs the time it takes to execute this operation. The .compute() method triggers the actual computation, as Dask operations are lazy and only execute when explicitly called.\n",
    " \n",
    "The \"CPU times\" indicates the total time the CPU spent processing the task, split between user-level operations and system-level operations. The \"Wall time\", being shorter than the CPU time, suggests that the task was executed in parallel across multiple CPU cores, thus completing faster than if run sequentially on a single core."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983ad10-a4c3-43b7-8514-a01c0c17cbac",
   "metadata": {},
   "source": [
    "Now, lets work on all the csv datasets that are inside the ***flights_data*** folder. This folder contains 500 csv files with 1000 rows each. Each csv file was randomly generated with flight information with the following columns:\n",
    "- **flight_id**: A unique identifier for each flight, generated sequentially for each row.\n",
    "- **origin**: The airport where the flight originates, randomly chosen from a list of airports (JFK, LGA, EWR).\n",
    "- **destination**: The airport where the flight is destined to land, randomly chosen from the same list of airports.\n",
    "- **airline**: The airline operating the flight, randomly chosen from a list of airlines (Delta, United, American).\n",
    "- **status**: The status of the flight, indicating whether it is 'On Time', 'Delayed', or 'Cancelled', randomly assigned.\n",
    "- **delay_minutes**: The number of minutes the flight is delayed, randomly assigned a value between 0 and 120.\n",
    "- **num_passengers**: The number of passengers on the flight, randomly assigned a value between 50 and 300.\n",
    "- **distance**: The distance the flight will travel, randomly assigned a value between 100 and 5000 miles.\n",
    "- **flight_duration**: The duration of the flight in minutes, randomly assigned a value between 30 and 600 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09e8519a-ac99-4443-b2d0-25b768f17fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b06d7551-8763-4f74-911f-d11652d864b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the generated flight CSV files\n",
    "#Your code goes here\n",
    "\n",
    "# Read the CSV files using Dask, with appropriate parsing options\n",
    "#Your code goes here\n",
    "\n",
    "# Display the Dask DataFrame\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd4c23-8a2f-48e2-b1fa-414b92d71077",
   "metadata": {},
   "source": [
    "When you print a Dask DataFrame (ddf), it does not show the actual data but rather provides a summary of the DataFrame. This is because Dask is designed to handle large datasets that might not fit into memory, so it uses lazy evaluation. This means that Dask does not immediately compute the results of the operations you define. Instead, it builds a task graph and waits until you explicitly trigger a computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1cb0b2-4c1c-4cc8-95e4-165bafc97a72",
   "metadata": {},
   "source": [
    "To view the actual data in a Dask DataFrame, you need to trigger a computation. Just like in pandas, we can use the ***head()*** method to view the first rows of the Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4476aae9-8149-4a14-b666-df89d34bf5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e38b9-6994-4c3f-8b58-757894e1b8a7",
   "metadata": {},
   "source": [
    "We can check the columns names and dtypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6135ea5d-5e94-4f8f-88b3-bf4657f6baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fcba715-e92c-43ed-aad2-e1bd700795a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880631b-74f2-4a1a-9e74-de158cc2b29a",
   "metadata": {},
   "source": [
    "Dask Dataframes also have an **.npartitions** attribute which tell us how many Pandas Dataframes make up a Dask Dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a0cd453-c671-4fff-a06f-45990666c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf14484-4d9b-4969-9d0e-502b10fcaed8",
   "metadata": {},
   "source": [
    "### Computations with Dask DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3850378-25d2-4e0d-a109-5cf41714017c",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics:\n",
    "Same as we learned with pandas, let's compute the mean, standard deviation, minimum, and maximum values across different numerical columns using the **describe()** method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd718294-5474-4253-adbf-bbe220e21477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1279654-d0fb-4126-94c9-2eec156164ba",
   "metadata": {},
   "source": [
    "#### Frequency Counts:\n",
    "Let's count how many flights were operated by the different airlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "552d7875-5f60-4065-a216-1902b2b4d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66e284-3609-42d8-a0cf-9d12c829b676",
   "metadata": {},
   "source": [
    "#### Group by Operations:\n",
    "Grouping data can provide insights into specific subsets. For example, to find out which airline, on average, experiences the most delays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf0f4a72-5cf1-4785-b35b-f5b692d5b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352996d1-b363-412f-82d2-395fda97a086",
   "metadata": {},
   "source": [
    "#### Exercise 3 (Breakout rooms)\n",
    "- Perform Basic Statistics:\n",
    "     - Calculate the mean and standard deviation of delay_minutes and flight_duration.\n",
    "- Advanced Calculations:\n",
    "    - Calculate the average number of passengers per flight for each airline.\n",
    "    - Determine the airline with the highest average delay.\n",
    "    - Calculate the total distance flown by all flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af810b87-3330-4ee7-922e-e78eb9dc09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Perform Basic Statistics\n",
    "# Calculate mean, median, and standard deviation for delay_minutes and flight_duration\n",
    "#Your code goes here\n",
    "\n",
    "# Step 2: Advanced Calculations\n",
    "# Average number of passengers per flight for each airline\n",
    "#Your code goes here\n",
    "\n",
    "# Airline with the highest average delay\n",
    "#Your code goes here\n",
    "\n",
    "# Total distance flown by all flights\n",
    "#Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
